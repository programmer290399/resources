{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HUSE: Hierarchical Universal Semantic Embeddings in PyTorch.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1wXDgQgBQRLGk3hiIrwRgjDKmtm1vZKwQ","authorship_tag":"ABX9TyP0hluuYWnBQcsI6f21FvdR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"OALLAojOm_c1","colab_type":"text"},"source":["# Implementation of [HUSE: Hierarchical Universal Semantic Embeddings](https://arxiv.org/pdf/1911.05978.pdf) in PyTorch\n","<hr></hr>\n","<img src=\"https://raw.githubusercontent.com/programmer290399/resources/master/HUSE.png\">\n","<hr></hr>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YicHwV-myXzl","colab_type":"text"},"source":["## <ins>Introduction</ins>:\n","* We, human beings use multiple senses to understand and make sense of the world around us. We use sight, hearing, smell, taste and touch to interact and understand things.\n","\n","* Taking input data of different form or say multiple modalities helps us understand and act better because data from different modalities share latent correlations. \n","\n","* To help computers understand we can use a similar approach called multi-modal learning.\n","\n","\n","* HUSE: Hierarchical Universal Semantic Embeddings paper proposes a multi-modal deep-learning method which enables multimodal em-beddings to share a common latent space.\n","\n","* HUSE projects images and text into a shared latent space by using a shared classification layer for image and text modalities.\n","\n","* HUSE incorporates semantic information by making the distance between any two universal embeddings to be similar to that of the distance between their class label embeddings in the semantic embedding space.\n","\n","* One thing where this method stands out is it not only allow the embeddings corresponding to a semantic class to lie closer to each other than the embeddings corresponding to two different classes but also makes learned universal embedding space semantically meaningful by clustering related classes closer than the unrelated ones.\n","\n","* A very good example of the above property can be found in Section 1 Para 3 of the paper.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Exyzvnh2zXRj","colab_type":"code","colab":{}},"source":["! unzip /content/drive/My\\ Drive/GreenDeck\\ ML\\ Assignment/images.zip\n","! pip install num2words nltk pytorch-pretrained-bert wget\n","! python -c \"import nltk ; nltk.download('all')\"\n","! pip install -U sentence-transformers"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GwbvDBev3-Ld","colab_type":"text"},"source":["## Imports \n","<hr></hr>\n","Here we import all the required packages"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vyD4SkuOvIMg","colab":{}},"source":["import wget\n","import torch\n","import pickle\n","import requests\n","import numpy as np\n","import progressbar\n","import pandas as pd\n","from torch import nn\n","from PIL import Image\n","from string import punctuation\n","from collections import Counter\n","from torch.optim import RMSprop\n","from num2words import num2words\n","import torch.nn.functional as F\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","from torchvision import models, transforms\n","from itertools import product, combinations\n","from torch.utils.data import Dataset, DataLoader\n","from sentence_transformers import SentenceTransformer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"noQBcC1S6cBI","colab_type":"text"},"source":["## Config Class \n","<hr></hr>\n","This class is used to set all the hyper parameters and other variable values to run the notebook.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"6ExMbZ4V6ro-","colab_type":"code","colab":{}},"source":["class config():\n","  \"\"\"\n","  A simple class for setting all the Hyperparameters and other configs \n","  for running the notebook.\n","  \"\"\"\n","\n","  def __init__(self):\n","\n","    # For ImageEmbeddingGen:\n","    # =====================\n","    # This model would be used to generate Image embeddings for\n","    # image tower you can see a complete list of models on :\n","    # https://pytorch.org/docs/stable/torchvision/models.html\n","    # NOTE : Please update the output dimensions if you change the model.\n","    self.ImageEmbdModel = models.vgg19(pretrained=True)\n","    self.ImageEmbd_out = (1,1000)\n","\n","\n","    # For TextEmbeddingGen:\n","    # ====================\n","    # This tokenizer and model would be used to generate Text embeddigs for\n","    # text tower you can see a complete list of models on :\n","    # https://pypi.org/project/pytorch-pretrained-bert/#doc\n","    # NOTE : Please update the output dimensions if you change the model.\n","    self.Tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n","    self.TxtEmbdModel = BertModel.from_pretrained('bert-base-uncased')\n","    self.TxtEmbd_out = (1,768)\n","\n","\n","    # For TextTower:\n","    # =============\n","    # The parameters up here are exactly as mentioned in section 4.1.3 \n","    # of the paper , the magic number 11127 below is actually the dimension \n","    # of the tf-idf vector which is concatenated with the bert output\n","    self.TxtTower_input = self.TxtEmbd_out[1] + 11127\n","    self.TxtTower_hidden = 512 \n","    self.TxtTower_dropout = 0.15\n","    self.TxtTower_output = 512\n","\n","\n","    # For ImageTower:\n","    # ==============\n","    # The parameters up here are exactly as mentioned in section 4.1.3\n","    self.ImgTower_input = self.ImageEmbd_out[1] \n","    self.ImgTower_hidden = 512 \n","    self.ImgTower_dropout = 0.15\n","    self.ImgTower_output = 512\n","\n","\n","    # Data Paths:\n","    # ===========\n","    # The paths for the training data csv and images can be set below \n","    self.csv_path = '/content/drive/My Drive/GreenDeck ML Assignment/training_data.csv'\n","    self.images_folder = '/content/content/netaporter_gb_images/'\n","    self.total_classes = 271 # As per the greendeck dataset provided.\n","\n","\n","    # For HUSEClassifier:\n","    # ===================\n","    # The paramerters up here are according to the description given in   \n","    # section 3.2 last para \n","    self.Huse_input = self.ImgTower_output + self.TxtTower_output\n","    self.Huse_output = self.total_classes\n","\n","\n","    # Hyperparameters:\n","    # ================\n","    # Set as described in section 4.1.3\n","    self.lr = 1.6192e-05\n","    self.num_epoch = 15 \n","    self.batch_size = 32 \n","    self.momentum = 0.9\n","    self.α = 1/3  # Classification loss coeff. # Keeping all the three same for now \n","    self.β = 1/3  # Semantic Similarity loss coeff.\n","    self.γ = 1/3  # Cross Modal loss coeff.\n","    self.ζ = 0.9  # Relaxation coeff.\n","\n","    # Training Device:\n","    # ================ \n","    self.device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Download Semantic graph\n","    # =======================\n","    # Set the below variable True if you wish to download and load semantic \n","    # graph from my github repo otherwise set it to false\n","    self.download_semantic_graph = True\n","# Created a config object for passing to all other classes later \n","config = config()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4b0mAnQJyyaB","colab_type":"text"},"source":["few utility functions below"]},{"cell_type":"code","metadata":{"id":"3YgCi3qaQaKn","colab_type":"code","colab":{}},"source":["def save_obj(obj, name):\n","  \"\"\"\n","  A simple method for saving a python object to a file for later use. \n","  \n","  Parameters\n","  ----------\n","  obj : Any python object, Required \n","    You can pass python objects like lists, dicts, etc.\n","  \n","  name : str, Required\n","    The pickle file of the object would be saved with this name  \n","  \"\"\"\n","\n","  with open( name + '.pkl', 'wb+') as f:\n","    pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n","\n","def load_obj(name):\n","  \"\"\"\n","  A simple method for loading a saved python object from a .pkl \n","  file.\n","\n","  Parameters\n","  ----------\n","  name : str, Required \n","    The name of the saved pickle file without the \".pkl\" at the end.\n","\n","  Returns\n","  -------\n","  Python object \n","    The python object is loaded from the provided filename and \n","    returned as it is.\n","  \"\"\"\n","\n","  with open(name + '.pkl', 'rb') as f:\n","    return pickle.load(f)\n","\n","def get_semantic_graph(classes_list, Download=config.download_semantic_graph):\n","  \"\"\"\n","  A function for getting semantic graph for later use.\n","\n","  Parameters\n","  ----------\n","  classes_list : iterable(list/numpy array), Required\n","    This should contain all the N classes\n","  Download : bool, optional\n","    If set to true , the function downloads the semantic graph and semantic \n","    graph map from my github repo to save computation time.\n","\n","  Returns\n","  -------\n","  semantic_graph : torch.tensor[N,N]\n","    The semantic graph is actually the adjacency matrix where each element on \n","    row i and column j represents the cosine distance between the embeddings \n","    of i th and j th classes.\n","  classes_combs_map : dict\n","    This dict contains the mapping of class name combinations and the indices \n","    where we can find the cosine distance corresponding to the combination in \n","    the semantic_graph \n","  \"\"\"\n","\n","  if Download :\n","\n","    urls = ['https://github.com/programmer290399/resources/raw/master/Semantic_graph.pth',\n","            'https://github.com/programmer290399/resources/raw/master/semantic_graph_map_new.pkl']\n","    \n","    for url in urls:\n","      wget.download(url)\n","    \n","    semantic_graph = torch.load('Semantic_graph.pth',map_location=torch.device(config.device))\n","    class_comb_map = load_obj('semantic_graph_map_new')\n","  \n","  else :\n","    \n","    semantic_graph, class_comb_map = SemanticGraph(classes_list) \n","\n","  return semantic_graph, class_comb_map\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S11vDZdUgK3D","colab_type":"text"},"source":["## PART1: CREATING TEXT AND IMAGE EMBEDDINGS INPUTS:\n","<hr></hr>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vGfySz17ubC_","colab_type":"text"},"source":["### <ins>Image Embeddings Input Model</ins>:\n","\n","<img src=\"https://github.com/programmer290399/resources/raw/master/image_tensor2Univ_embdpng.png\"/>\n","<p>Fig 1: Process of creating universal embeddings from Image tensor</p>\n","<hr>\n","\n","* To generate universal embedding for image HUSE uses a Backbone Image Network to convert Image Tensors to an intermediate embedding \n","\n","* This intermediate embedding is nothing but the output of the backbone network which is fed to Image Tower which generates the Universal Embeddings\n","\n","* The class ImageEmbeddingGen (given below) accomplishes the work of the backbone image network here as depicted by second block in the image above(Fig 1).\n","\n","* The model I've used here gives output embeddings of shape 1X1000 for an input image tensor\n"]},{"cell_type":"code","metadata":{"id":"Sg39gR_MSdpm","colab_type":"code","colab":{}},"source":["class ImageEmbeddingGen():\n","  \"\"\"\n","  A class to get Image embeddings from the desired model \n","  for feeding to the ImageTower \n","\n","  ...\n","  \n","  Attributes\n","  ----------\n","  model : torchvision.models.model\n","    The model from which the embeddings would be fetched\n","\n","  Methods\n","  -------\n","  get_ImgEmbd(image)\n","    Returns the image embedding of the passed image. \n","  \"\"\"\n","\n","  def __init__(self,config):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    config : config class object\n","      The model to be used would be fetched from config\n","    \"\"\"\n","\n","    self.model = config.ImageEmbdModel\n","    # Freezing model parameters and putting it in eval mode \n","    for param in self.model.parameters():\n","      param.requires_grad_(False)\n","    self.model.eval()\n","    self.model.to(config.device)\n","  \n","  def get_ImgEmbd(self,image):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    image : torch.tensor, Required \n","      The normalised image tensor for feeding to the \n","      model. \n","\n","    Returns \n","    -------\n","    ImgEmbd : torch.tensor[1,1000]\n","      The image embding generated by the model. \n","    \"\"\"\n","\n","    ImgEmbd = self.model(image)\n","    return ImgEmbd"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9CShXKDxD_xG","colab_type":"text"},"source":["### <ins>Text Embeddings Input Model</ins>:\n","\n","<img src=\"https://github.com/programmer290399/resources/raw/master/text2Univ_embdpng.png\"/>\n","<p>Fig 2: Process of creating universal embeddings from Text</p>\n","<hr>\n","\n","* To generate universal embedding for text HUSE uses a Backbone text Network to convert Text to an intermediate embedding. \n","\n","* This intermediate embedding is nothing but the output of the backbone network which is fed to Text Tower which generates the Universal Embeddings.\n","\n","* The class TextEmbeddingGen (given below) accomplishes the work of the backbone text network here as depicted by second block in the image above.(Fig 2)\n","\n","* We have used bert as the backbone text network and have mean pooled the ouput of last 4 layers.\n","\n","* The input actually fed to the Text Tower also contains the tf-idf vector of the text which is concatenated with the output of the backbone text network and then fed to it.\n","\n","* This network outputs embedding of shape 1X768 for a text input.\n"]},{"cell_type":"code","metadata":{"id":"BnOWSh_KU_fv","colab_type":"code","colab":{}},"source":["class TextEmbeddingGen():\n","  \"\"\"\n","  A class to get Text embeddings from the desired model \n","  for feeding to the TextTower\n","\n","  ...\n","\n","  Attributes\n","  ----------\n","  tokenizer : obj \n","    The bert tokenizer which tokenizes input text for futher processing\n","  model : torch.nn.Module\n","    The bert model for generating text embeddings \n","\n","  Methods\n","  -------\n","  get_TxtEmbd_bert(text)\n","    Returns the textembding generated mean pooling the output of last 4 layers\n","    of the bert model\n","  \"\"\"\n","\n","  def __init__(self,config):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    config : config class object\n","      The model to be used would be fetched from config\n","    \"\"\"\n","\n","    self.tokenizer = config.Tokenizer_bert\n","    self.model = config.TxtEmbdModel\n","    # Freezing model parameters and putting it in eval mode\n","    for param in self.model.parameters():\n","      param.requires_grad_(False)\n","    self.model.eval()\n","    self.model.to(config.device)\n","    \n","  def get_TxtEmbd_bert(self,text):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    text : str, Required \n","      The input text for which we need to generate embeddings\n","    \n","    Returns\n","    -------\n","    mean_pooled_last_4_tokens : torch.tensor[1,768]\n","      The mean pooled output of last 4 layers.\n","    \"\"\"\n","\n","    # Tokenizing the input text and generating token and segment IDs\n","    tokenized_text = self.tokenizer.tokenize(\"[CLS] \" +text+ \" [SEP]\" )\n","    indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n","    segments_ids = [1] * len(tokenized_text)\n","    \n","    # Converting indexes tokens and segment IDs to tensors for further \n","    #operations\n","    tokens_tensor = torch.tensor([indexed_tokens]).to(config.device)\n","    segments_tensors = torch.tensor([segments_ids]).to(config.device)\n","    \n","    # Getting the output from the model\n","    with torch.no_grad():\n","      encoded_layers, _ = self.model(tokens_tensor, segments_tensors)\n","    \n","    # Here we stack the layers and mean pool them \n","    last_4_layer_out = torch.stack(encoded_layers[-4:])\n","    meanpooled_last_4 = torch.squeeze(torch.mean(last_4_layer_out, dim=0))\n","    mean_pooled_last_4_tokens = torch.mean(meanpooled_last_4,dim=0)\n","    mean_pooled_last_4_tokens = mean_pooled_last_4_tokens.reshape(1,768)\n","    return mean_pooled_last_4_tokens\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sMEtBYGid_41","colab_type":"text"},"source":["## PART2: MODEL IMPLEMENTATION FOR CREATING FINAL EMBEDDINGS:\n","<hr></hr>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZnsQgse-uRAM","colab_type":"text"},"source":["### <ins>Text Tower Model</ins>:\n","\n","* This model takes text embedding generated by the backbone text network with the tf-idf vector of the same text to generate universal embeddings for it.\n","\n","* This is depicted by third block in Fig 2.\n","\n","* This class, TextTower generates universal embeddings of shape 1X512, though this can be changed in the config class definition.(as quoted in section 3.2 para 2 of the paper)(given below)\n","\n","  >\"the backbone text network\n","followed by text tower corresponds to an text projection\n","function φ T (·) ∈ R D that projects text to D-dimensional\n","universal embedding space.\"\n","\n","\n","* It is structured exactly as described in section 4.1.3 of the paper."]},{"cell_type":"code","metadata":{"id":"xQWHm8m-yq3G","colab_type":"code","colab":{}},"source":["class TextTower(nn.Module):\n","  \"\"\"\n","  A class for taking text embeddings and converting them to a fixed D \n","  dimensional universal embedding.\n","\n","  ...\n","\n","  Attributes\n","  ----------\n","  input_size : int \n","    The input size of the tower \n","  hidden_size : int \n","    The size of the hidden layer\n","  output_size : int \n","    Output dimension as required \n","  dropout : float \n","    The dropout prob to be used \n","  fc_inp : torch.nn.Linear\n","    The fully connected layer which takes input embedding \n","  fc_out : torch.nn.Linear\n","    The fully connected layer which gives output embedding \n","\n","  Methods\n","  -------\n","  forward(input_embd)\n","    Automatically called when we pass text embedding to class obj, \n","    returns universal embedding of the text embedding passed.  \n","  \"\"\"\n","\n","  def __init__(self, config):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    config : config class object\n","      All the attricutes are fetched from this object\n","    \"\"\"\n","\n","    super().__init__()\n","    self.input_size  = config.TxtTower_input\n","    self.hidden_size = config.TxtTower_hidden\n","    self.output_size = config.TxtTower_output\n","    self.dropout     = nn.Dropout(p=config.TxtTower_dropout)\n","    self.fc_inp      = nn.Linear(self.input_size, self.hidden_size)\n","    self.fc_out      = nn.Linear(self.hidden_size,self.output_size)\n","\n","  def forward(self, input_embd):\n","    \"\"\"\n","    Parameters \n","    ----------\n","    input_embd : torch.tensor[1, input_size], Required \n","      This is the concatenation of the bert embeddings and the tf-idf vector \n","\n","    Returns\n","    -------\n","    output : torch.tensor[1,output_size]\n","      The universal embedding of the text.\n","    \"\"\"\n","\n","    x = self.dropout(F.relu(self.fc_inp(input_embd)))\n","    output = F.normalize(F.relu(self.fc_out(x)))\n","    \n","    return output\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dZkAh8mxIxYn","colab_type":"text"},"source":["### <ins>Image Tower Model</ins>:\n","\n","* This model takes image embedding generated by the backbone image network to generate universal embeddings for it.\n","\n","* This is depicted by third block in Fig 1.\n","\n","* This class, ImageTower generates universal embeddings of shape 1X512, though this can be changed in the config class definition.(as quoted in section 3.2 para 1 of the paper)(given below) \n","\n","  >\"HUSE consists of an image tower that returns universal\n","  embeddings corresponding to an image.\"\n","\n","  >\"the backbone image net-\n","work followed by the image tower corresponds to the image\n","projection function φ I (·) ∈ R D that projects an image to\n","D-dimensional universal embedding space.\"\n","\n","* It is structured exactly as described in section 4.1.3 of the paper."]},{"cell_type":"code","metadata":{"id":"dVvxk0RAyqrX","colab_type":"code","colab":{}},"source":["class ImageTower(nn.Module):\n","  \"\"\"\n","  A class for taking Image embeddings and converting them to a fixed D \n","  dimensional universal embedding.\n","\n","  ...\n","\n","  Attributes\n","  ----------\n","  input_size : int \n","    The input size of the tower \n","  hidden_size : int \n","    The size of the hidden layer\n","  output_size : int \n","    Output dimension as required \n","  dropout : float \n","    The dropout prob to be used \n","  fc_inp : torch.nn.Linear\n","    The fully connected layer which takes input embedding\n","  fc_hidden_1 : torch.nn.Linear\n","    Fully connected hidden layer\n","  fc_hidden_2 : torch.nn.Linear\n","    Fully connected hidden layer \n","  fc_hidden_3 : torch.nn.Linear\n","    Fully connected hidden layer   \n","  fc_out : torch.nn.Linear\n","    The fully connected layer which gives output embedding \n","\n","  Methods\n","  -------\n","  forward(input_embd)\n","    Automatically called when we pass image embedding to class obj, \n","    returns universal embedding of the image embedding passed.  \n","  \"\"\"\n","\n","  def __init__(self,config):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    config : config class object\n","      All the attricutes are fetched from this object\n","    \"\"\"\n","\n","    super().__init__()\n","    self.input_size  = config.ImgTower_input\n","    self.hidden_size = config.ImgTower_hidden\n","    self.output_size = config.ImgTower_output\n","    self.dropout     = nn.Dropout(p=config.ImgTower_dropout)\n","    self.fc_inp      = nn.Linear(self.input_size, self.hidden_size)\n","    self.fc_hidden_1 = nn.Linear(self.hidden_size, self.hidden_size)\n","    self.fc_hidden_2 = nn.Linear(self.hidden_size, self.hidden_size)\n","    self.fc_hidden_3 = nn.Linear(self.hidden_size, self.hidden_size)\n","    self.fc_out      = nn.Linear(self.hidden_size,self.output_size)\n","  \n","  def forward(self, input_embd):\n","    \"\"\"\n","    Parameters \n","    ----------\n","    input_embd : torch.tensor[1, input_size], Required \n","      The image embding for which we need universal embedding.\n","\n","    Returns\n","    -------\n","    output : torch.tensor[1,output_size]\n","      The universal embedding of the image.\n","    \"\"\"\n","\n","    x = self.dropout(F.relu(self.fc_inp(input_embd)))\n","    x = self.dropout(F.relu(self.fc_hidden_1(x)))\n","    x = self.dropout(F.relu(self.fc_hidden_2(x)))\n","    x = self.dropout(F.relu(self.fc_hidden_3(x)))\n","    output = F.normalize(F.relu(self.fc_out(x)))\n","\n","    return output"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h2U_TegfKiR6","colab_type":"text"},"source":["### <ins>Shared Classification Layer</ins>:\n","<img src=\"https://github.com/programmer290399/resources/raw/master/classifier_layer.png\"/>\n","Fig 3: Process of classification from universal embeddings of text and image\n","<hr></hr>\n","\n","* This layer takes in the universal embeddings and classifies them into a target class \n","* This is made as per the specifications in section 3.2 para 4.\n","* The process is clearly depicted in the image above fig 3.\n",">NOTE : It was a bit unclear to me that how the embeddings would be fed to the classification layer , there are two possible ways, one is which I have implemented and shown above in the image, the other is when we feed them one by one and take average of the logits and then classify. \n"]},{"cell_type":"code","metadata":{"id":"9gLXsIaKeMsb","colab_type":"code","colab":{}},"source":["class HUSEClassifier(nn.Module):\n","  \"\"\"\n","  A shared classification layer for the universal embeddings. \n","\n","  ...\n","\n","  Attributes\n","  ----------\n","  input_size : int \n","    The input size of the layer\n","  output_size : int \n","    The input size of the layer\n","  shared_layer : torch.nn.Linear\n","    The fully connected layer for classification\n","\n","  Methods\n","  -------\n","  forward(input_embd)\n","    Automatically called when we pass Universal embedding to class obj, \n","    returns classification of the Universal embedding passed.\n","  \"\"\"\n","\n","  def __init__(self, config):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    config : config class object\n","      All the attributes are fetched from this object\n","    \"\"\"\n","\n","    super().__init__()\n","    self.input_size = config.Huse_input\n","    self.output_size = config.Huse_output\n","    self.shared_layer = nn.Linear(self.input_size,self.output_size)\n","  \n","  def forward(self,input_embd):\n","    \"\"\"\n","    Parameters \n","    ----------\n","    input_embd : torch.tensor[1, input_size], Required \n","      The universal embding for which we need classification.\n","\n","    Returns\n","    -------\n","    output : torch.tensor[1,output_size]\n","      The classification output.\n","    \"\"\"\n","\n","    output = self.shared_layer(input_embd)\n","    \n","    return output\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AKPtLtCkkkx4","colab_type":"text"},"source":["## PART3: INCORPORATING THREE  LOSSES INTO THE ARCHITECTURE:\n","<hr></hr>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WYPR2S4ZusIO","colab_type":"text"},"source":["### <ins>Class Level Similarity and Classification Loss</ins>:\n","\n","* This is perhaps the simplest and quite common loss type, here I've used the softmax cross entropy loss as described in 3.4.1 of the paper.\n","* This loss is implemented using nn.CrossEntropyLoss from pytorch\n","* This loss tries to can cluster the embeddings corresponding to a class together."]},{"cell_type":"markdown","metadata":{"id":"e3W5RGytur6G","colab_type":"text"},"source":["### <ins>Semantic Similarity, Semantic Graph, and Semantic Loss</ins>:\n","\n","#### <ins>Semantic Graph</ins>:\n","<img src=\"https://github.com/programmer290399/resources/raw/master/semantic_graph_illustration.jpeg\" width=\"500\" height=\"500\"/>\n","<p>Fig 4: Semantic Graph Visualization</p>\n","<hr></hr>\n","\n","* The semantic graph is basically a Adjacency matrix which can be visualized very well in Fig 4 above.\n","* Say we have five classes, Class A, Class B, Class C, Class D and Class E.\n","  So in the semantic graph these classes form the vertices of the graph and each vertice is connected to all other vertices. \n","\n","* The edges connecting any two classes are weighted with the distance(cosine distance in our case) between the embeddings of the class names.\n","\n","* Say Class A has an index i in the class list and Class B has index j. Then the semantic graph's value on i,j would be the distance between the embeddings of the class names.\n","\n","* More formally as stated in the paper section 3.3:\n",">\"we define the semantic graph as G =\n","(V, E), where V = {v 1 , v 2 , ..., v K } represents the set of K\n","classes and E represent the edges between any two classes.\n","Let ψ(·) represent the function that extracts embeddings of a\n","class name.\"\n","\n","* The element A[i][j] is described as by equation (4) in section 3.3 of the paper.\n","\n","\n","<img src=\"https://github.com/programmer290399/resources/raw/master/semantic_compute_illustration.jpeg\" width=\"600\" height=\"400\"/>\n","<p>Fig 5: Semantic Graph Visualization</p>\n","<hr></hr>\n","\n","* The illustration above in Fig 5 shows how we compute semantic similarity between our classes.\n","* We split the class name into appropriately into sub tokens \n","* Then we generate their respective embeddings using bert \n","* Then we take the mean of their embeddings\n","* Finally we compute the cosine similarity between the two and\n","* Assign it to the designated position in our semantic graph\n","\n","\n","#### <ins>Semantic Loss</ins>:\n","<img src=\"https://github.com/programmer290399/resources/raw/master/semantic_loss.jpeg\" width=\"800\" height=\"700\"/>\n","<p>Fig 6: Semantic Loss Computation</p>\n","<hr></hr>\n","\n","* Above you can see in Fig 5 the process of semantic loss computation which was stated in the paper as shown below :\n","><img src=\"https://github.com/programmer290399/resources/raw/master/semantic_loss_equations.png\"/>\n","\n","* So we start with the universal embeddings we computed using text and image towers with their respective class labels.\n","* The next step is to pick unique pairs of embeddings and their respective classes.\n","* Then there are two branches, one is the set of pairs of class labels which are sent to semantic graph map , (which is nothing but the dictionary which stores the index vaues where the already computed class embedding similarity can be found) from where we get the indexes and use them to fetch the similarity value (which is the cosine similarity between them)(as referred in the eqn by Aij) from the semantic graph.\n","* The other branch simultaneously computes the cosine similarity between the embedding pairs we formed earlier.\n","* Then two branches merge and check whether both the embedding pair similarity value and the one fetched from semantic graph are lesser than the margin \n","* According to which we decide the value of sigma (which is shown in the equation above by σ) \n","* Then we compute the pair wise difference and square it as per the loss equation and multiply it with σ\n","* Finally we divide it by N^2 to complete the computation. \n","* This loss tries to enforce semantic graph regularization , which would try that embeddings corresponding to two semantically similar classes are closer than the embeddings corresponding to two semantically different classes  \n","\n"]},{"cell_type":"markdown","metadata":{"id":"Wics52iiursq","colab_type":"text"},"source":["### <ins>Cross Modal Gap and Cross Modal Loss</ins>:\n","<img src=\"https://github.com/programmer290399/resources/raw/master/cross_modal_loss.jpeg\" width=\"500\" height=\"500\"/>\n","<p>Fig 7: Cross Modal Computation</p>\n","<hr></hr>\n","\n","* Cross modal loss computation is pretty simple we just pass the universal embedding of text and image of same instance to cosine similarity function and take the avg as defined by the eqn (12) in the paper.\n","* This is done because afterall both the embeddings represent the same things and thus they must be as close as possible.\n"]},{"cell_type":"code","metadata":{"id":"rw3hQl_2kmWI","colab_type":"code","colab":{}},"source":["def SemanticGraph(classes_list,config=config):\n","  \"\"\"\n","  A function to create semantic graph from list of all unique classes, as \n","  described in section 3.3 of the paper.\n","  NOTE: If you run this function please run it on GPU as it takes only 15-20 min\n","  to run , on CPU it may take upto 2 hours.\n","  \n","  Parameters\n","  ----------\n","  classes_list : iterable(list/numpy array), Required\n","    This should contain all the N classes \n","  config : config class object, Optional\n","    This is used to fetch the device\n","  \n","  Returns\n","  -------\n","  semantic_graph : torch.tensor[N,N]\n","    The semantic graph is actually the adjacency matrix where each element on \n","    row i and column j represents the cosine distance between the embeddings \n","    of i th and j th classes.\n","  classes_combs_map : dict\n","    This dict contains the mapping of class name combinations and the indices \n","    where we can find the cosine distance corresponding to the combination in \n","    the semantic_graph \n","  \"\"\"\n","\n","  # Here we split the class names and attach index to them for mapping \n","  # combinations later \n","  classes_list_split = [class_name.split('<')+[idx] for idx,class_name in enumerate(classes_list)]\n","  \n","  # This function is to reconstruct the original class names for mapping them \n","  # later\n","  reconstruct_keys = lambda class_prod : \"|\".join(map(lambda inp: \"<\".join(inp[:-1]),class_prod))\n","  \n","  # Fetching total number of classes\n","  num_classes = len(classes_list_split)\n","  \n","  # Loading bert model for embedding generation and moving it to the device\n","  # For more info see : https://github.com/UKPLab/sentence-transformers\n","  model = SentenceTransformer('bert-base-nli-mean-tokens')\n","  model = model.to(config.device)\n","  \n","  # Initializing empty semantic graph and class_combination_map\n","  semantic_graph = torch.zeros(num_classes,num_classes).to(config.device)\n","  classes_combs_map = dict()\n","  \n","  # Here we compute all the unique combinations so that we can compute \n","  # distance efficiently \n","  classes_combs = combinations(classes_list_split,r=2)\n","  \n","  # The magic number below 36585 is nothing but the total number of combinations\n","  # of the 271 classes , more precisely C(271,2)=36585\n","  with progressbar.ProgressBar(max_value=36585) as bar:\n","\n","    # Here we iterate over the above computed unique combinations\n","    for _,class_prod in enumerate(classes_combs):\n","\n","      # Getting the class indices for mapping \n","      i,j = class_prod[0][-1],class_prod[1][-1]\n","\n","      # A lambda function to get embedding for classes\n","      get_embd = lambda x : torch.tensor(model.encode(x)).to(config.device)\n","      \n","      # Generating embedding for the class names \n","      emb1,emb2 = get_embd(class_prod[0][:-1]), get_embd(class_prod[1][:-1])\n","\n","      # Computing cosine similarity between the two embeddings generated above\n","      cos_sim = F.cosine_similarity(emb1.reshape(1,-1),emb2.reshape(1,-1))\n","      \n","      # Assigning the values in the graph and map, note that our all \n","      #combinations are unique and thus if we have (i,j) as a combination we \n","      # don't have (j,i) neither we compute it to save time as it is the same.\n","      semantic_graph[i][j] = semantic_graph[j][i] = cos_sim\n","      classes_combs_map[reconstruct_keys(class_prod)] = (i,j)\n","      \n","      bar.update(_)       \n","  \n","  # We didn't compute similarities for cases where i == j as it is always 1 for \n","  # cosine similarity so we fill those indices in the graph and map as well \n","  semantic_graph[np.diag_indices_from(semantic_graph)] = 1.0\n","  for i,class_name in enumerate(classes_list):\n","    classes_combs_map[class_name +'|'+ class_name] = (i,i)\n","\n","  return semantic_graph, classes_combs_map\n","  \n","class Loss():\n","  \"\"\"\n","  A class for calculation of various losses as mentioned in the paper\n","\n","  ...\n","\n","  Attributes\n","  ----------\n","  alpha : float\n","    The coeff. for balancing classification loss\n","  beta : float \n","    The coeff. for balancing semantic loss\n","  gamma : float \n","    The coeff. for balancing cross modal loss\n","  margin : float \n","    margin for Relaxation coeff.for enforcing regularization of semantic classes \n","    which are closer than the margin and make other embedding pairs at least as \n","    large as the margin.\n","  \n","  Methods\n","  -------\n","  ClassificationLoss(Huse_out, labels)\n","    Returns classification loss \n","  SemanticLoss(semantic_graph,classes_combs_map,Univ_embd_batch,Batch_labels,margin)\n","    Returns semantic loss as described in section 3.4.2 of the paper\n","  CrossModalLoss(Univ_txtEmbd, Univ_imgEmbd)\n","    Returns cross modal loss as desctibed in section 3.4.3 of the paper\n","  TotalLoss(ClassificationLoss,SemanticLoss,CrossModalLoss)\n","    Returns the final loss as given by equation (5) in the paper\n","  \"\"\"\n","\n","  def __init__(self,config):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    config : config class object\n","      All the attributes are fetched from this object\n","    \"\"\"\n","\n","    self.alpha = config.α\n","    self.beta = config.β\n","    self.gamma = config.γ\n","    self.margin = config.ζ\n","\n","  def ClassificationLoss(self,Huse_out, labels):\n","    \"\"\"\n","    Parameters\n","    ----------\n","      Huse_out : torch.tensor[BS,N], Required\n","        ouput of the classification layer where BS is batch size and N is total\n","        number of classes \n","      labels : torch.tensor[BS], Required\n","        the corresponding labels from the dataset\n","    \n","    Returns\n","    -------\n","      criterion(Huse_out, labels) : torch.tensor\n","        The cross entropy loss\n","    \"\"\"\n","\n","    criterion = nn.CrossEntropyLoss()\n","    \n","    return criterion(Huse_out, labels)\n","\n","  def SemanticLoss(self,semantic_graph,classes_combs_map,Univ_embd_batch,\n","                   Batch_labels,margin=None):\n","    \"\"\"\n","    Paramerters\n","    -----------\n","      semantic_graph : torch.tensor, Required\n","        The semantic graph as generated by the SemanticGraph function \n","      classes_combs_map : dict, Required  \n","        The class combination to index mapping for semantic graph as returned by\n","        SemanticGraph function\n","      Univ_embd_batch : torch.tensor, Required\n","        The batch of universal embeddings created by text or image tower \n","      Batch_labels : torch.tensor, Required \n","        The class labels of the batch.\n","      margin : float, Optional \n","        margin as discussed above in class attributes description  \n","    \n","    Returns\n","    -------\n","      loss : torch.tensor \n","        The semantic loss as described above in Methods description\n","    \"\"\"\n","\n","    # We fetch the batch size and margin if not passed \n","    batch_size = Univ_embd_batch.shape[0] \n","    margin = self.margin if margin is None else margine\n","\n","    # We create unique combinations of indexes and compute its length\n","    idx_combinations = list(combinations(range(batch_size),r=2))\n","    num_combinations = len(idx_combinations)\n","    \n","    # We initialize 2 column vectors, cosine_similarity_vals & semantic_graph_vals \n","    # for storing the cosine similarity of the embedding combinations and the \n","    # semantic graph values of thier respective class combinations respectively. \n","    cosine_similarity_vals = torch.zeros(num_combinations,1,dtype=torch.float32).to(config.device)\n","    semantic_graph_vals = torch.zeros(num_combinations,1,dtype=torch.float32).to(config.device)\n","    \n","    # Iterating over the combinations computed above \n","    for i,idx_comb in enumerate(idx_combinations):\n","\n","      # fetching the indices and embedding values and reshaping them suitably \n","      m,n = idx_comb\n","      univ_embd_m,univ_embd_n = Univ_embd_batch[m].reshape(1,-1),Univ_embd_batch[n].reshape(1,-1)\n","      \n","      # Computing cosine similarity between the embedding combination and storing\n","      # it in the i th row of cosine_similarity_vals\n","      cosine_similarity = F.cosine_similarity(univ_embd_m,univ_embd_n)\n","      cosine_similarity_vals[i] = cosine_similarity\n","\n","      # Here we try to fetch the corresponding similarity value for class \n","      # combination say (A,B) but we're not sure that whether the key in the \n","      # classes_combs_map is A|B or B|A so we use a try except block\n","      try :\n","        batch_label_key = '|'.join([Batch_labels[m][0],Batch_labels[n][0]])\n","        j,k = classes_combs_map[batch_label_key]\n","      except:\n","        batch_label_key = '|'.join([Batch_labels[n][0],Batch_labels[m][0]])\n","        j,k = classes_combs_map[batch_label_key]\n","      \n","      # Storing the similarity value from semantic_graph in the i th row of \n","      # semantic_graph_vals\n","      semantic_graph_vals[i] = semantic_graph[j][k]\n","    \n","    # Initializing loss \n","    loss = 0 \n","\n","    # We form pairs by concatenating cosine_similarity_vals & semantic_graph_vals\n","    cosine_similarity_pairs = torch.cat([cosine_similarity_vals,semantic_graph_vals],dim=1).to(config.device)\n","\n","    # We iterate through the pairs and compute the relaxing constraint as per \n","    # the margin passed and compute the loss accordingly \n","    for pair in cosine_similarity_pairs :\n","      σ = 1 if all([pair[0]<margin , pair[1] < margin]) else 0 \n","      loss += σ * (pair[0]-pair[1])**2\n","    loss /= batch_size**2\n","    \n","    return loss\n","\n","  def CrossModalLoss(self, Univ_txtEmbd, Univ_imgEmbd):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    Univ_txtEmbd : torch.tensor, Required\n","      The universal text embedding generated by text tower \n","    Univ_imgEmbd : torch.tensor, Required\n","\n","    Returns\n","    -------\n","    loss : torch.tensor\n","      Cross modal loss as described above in Methods Description \n","    \"\"\"\n","\n","    return torch.mean(F.cosine_similarity(Univ_imgEmbd, Univ_txtEmbd))\n","\n","  def TotalLoss(self,ClassificationLoss,SemanticLoss,CrossModalLoss):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    ClassificationLoss : torch.tensor, Required \n","      output of ClassificationLoss function \n","    SemanticLoss : torch.tensor, Required\n","      output of SemanticLoss function \n","    CrossModalLoss : torch.tensor, Required\n","      output of CrossModalLoss function \n","    \n","    Returns\n","    -------\n","    loss as described above in Method description\n","    \"\"\"\n","\n","    return (self.alpha*ClassificationLoss) + (self.beta * SemanticLoss) + (self.gamma * CrossModalLoss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J4eo7-nDvq_G","colab_type":"text"},"source":["## PART4: TRAINING \n","<hr></hr>\n","Below you can find a custom dataset class to load the provided dataset and a function to train the networks."]},{"cell_type":"code","metadata":{"id":"XJ8Ccs7Klc1l","colab_type":"code","colab":{}},"source":["class GreenDeckDataset(Dataset):\n","  \"\"\"\n","  A custom dataset class to load, clean and preprocess the provided dataset  \n","  \"\"\"\n","\n","  def __init__(self,config):\n","    self.data = pd.read_csv(config.csv_path)\n","    self.img_root = config.images_folder\n","    self.class_list = self.data.classes.unique()\n","    self.tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n","    self.docs = list(self.data.text)\n","    self.tfidf_vectors = self.tfidf_vectorizer.fit_transform(self.docs)\n","\n","  def __getitem__(self, index):\n","    image_path = self.img_root + self.data.loc[index, 'image']\n","    image = self._load_image(image_path)\n","\n","    text_raw = self.data.loc[index,'text']\n","    text_cleaned  = self._preprocess_text(text_raw)\n","    class_name = self.data.loc[index,'classes']\n","    class_idx = np.where(self.class_list == class_name)\n","    tfidf_vector = torch.tensor(self.tfidf_vectors[index].toarray(),\n","                                dtype=torch.float32)\n","    \n","    return image,text_cleaned,class_name,tfidf_vector,class_idx\n","\n","  def __len__(self):\n","    return self.data.shape[0]\n","\n","  def _load_image(self,img_path, max_size=400, shape=None):\n","    # Load in and transform an image, making sure the image\n","    #  is <= 400 pixels in the x-y dims\n","    \n","    image = Image.open(img_path).convert('RGB')\n","    \n","    # large images will slow down processing\n","    if max(image.size) > max_size:\n","        size = max_size\n","    else:\n","        size = max(image.size)\n","    \n","    if shape is not None:\n","        size = shape\n","        \n","    in_transform = transforms.Compose([\n","                        transforms.Resize(size),\n","                        transforms.RandomRotation(degrees=15),\n","                        transforms.RandomHorizontalFlip(),\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.485, 0.456, 0.406), \n","                                             (0.229, 0.224, 0.225))])\n","\n","    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n","    image = in_transform(image)[:3,:,:].unsqueeze(0)\n","    \n","    return image\n","  \n","  def _convert_lower_case(self,data):\n","    # Convert text to all lower case \n","\n","    return np.char.lower(data)\n","\n","  def _remove_stop_words(self,data):\n","    # Remove stop words from text\n","\n","    stop_words = stopwords.words('english')\n","    words = word_tokenize(str(data))\n","    new_text = \"\"\n","    for w in words:\n","        if w not in stop_words and len(w) > 1:\n","            new_text = new_text + \" \" + w\n","    return new_text\n","  \n","  def _remove_punctuation(self,data):\n","    # Remove punctuations from text\n","\n","    for i in range(len(punctuation)):\n","        data = np.char.replace(data, punctuation[i], ' ')\n","        data = np.char.replace(data, \"  \", \" \")\n","    data = np.char.replace(data, ',', '')\n","    return data\n","\n","  def _remove_apostrophe(self,data):\n","    # Remove apostroshe from text \n","\n","    return np.char.replace(data, \"'\", \"\")\n","\n","  def _stemming(self,data):\n","    # stemming the given text\n","\n","    stemmer= PorterStemmer()\n","    \n","    tokens = word_tokenize(str(data))\n","    new_text = \"\"\n","    for w in tokens:\n","        new_text = new_text + \" \" + stemmer.stem(w)\n","    return new_text\n","  \n","  def _convert_numbers(self,data):\n","    # convert numbers to words eg. 101 --> one hundred and one\n","\n","      tokens = word_tokenize(str(data))\n","      new_text = \"\"\n","      for w in tokens:\n","          try:\n","              w = num2words(int(w))\n","          except:\n","              a = 0\n","          new_text = new_text + \" \" + w\n","      new_text = np.char.replace(new_text, \"-\", \" \")\n","      return new_text\n","    \n","  def _preprocess_text(self,data):\n","    # Use all other text cleaning methods to clean the text.\n","\n","    data = self._convert_lower_case(data)\n","    data = self._remove_punctuation(data) #remove comma seperately\n","    data = self._remove_apostrophe(data)\n","    data = self._remove_stop_words(data)\n","    data = self._convert_numbers(data)\n","    data = self._stemming(data)\n","    data = self._remove_punctuation(data)\n","    data = self._convert_numbers(data)\n","    data = self._stemming(data) #needed again as we need to stem the words\n","    data = self._remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n","    data = self._remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n","    \n","    return data  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8NSFkYjCtRdO","colab_type":"code","colab":{}},"source":["testds = GreenDeckDataset(config)\n","classes_list = testds.class_list\n","semantic_graph, class_comb_map = get_semantic_graph(classes_list) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZgMXDmLKglcU","colab":{}},"source":["def train(config         = config,\n","          dataset        = GreenDeckDataset(config),\n","          ImgEmbdGen     = ImageEmbeddingGen(config),\n","          TxtEmbdGen     = TextEmbeddingGen(config),\n","          TxtTower       = TextTower(config),\n","          ImgTower       = ImageTower(config),\n","          HUSEClassifier = HUSEClassifier(config), \n","          Loss           = Loss(config),\n","          semantic_graph = semantic_graph,\n","          classes_combs_map = class_comb_map):\n","  \"\"\"\n","  A function to train the HUSE networks parts viz. HUSEClassifier, TextTower &\n","  ImageTower\n","\n","  Parameters\n","  ----------\n","\n","  config : config class obj, Optional\n","    All the hyperparameters and other required info is fetched from this obj \n","  dataset : torch.utils.data.Dataset, Optional\n","    The complete function fetches data using this dataset obj \n","  ImgEmbdGen : ImageEmbeddingGen class obj, Optional\n","    This is used to convert images to input embeddings for image tower\n","  TxtEmbdGen : TextEmbeddingGen class obj, Optional\n","    This is used to convert text to input embeddings for text tower\n","  TxtTower : torch.nn.Module, Optional\n","    This converts text embeddings to universal embeddings\n","  ImgTower : torch.nn.Module, Optional\n","    This converts image embeddings to universal embeddings\n","  HUSEClassifier : torch.nn.Module, Optional \n","    This is the shared classification layer of the model\n","  Loss : Loss class obj, Optional\n","    This class incorporates all the 3 losses in it \n","  semantic_graph : torch.tensor, Optional \n","    The semantic graph as generated by the SemanticGraph function\n","  classes_combs_map : dict, Optional \n","    The class combination to index mapping for semantic graph as returned by\n","    SemanticGraph function\n","\n","  Returns\n","  -------\n","  TxtTower : torch.nn.Module\n","    The trained text tower \n","  ImgTower : torch.nn.Module \n","    The trained image tower \n","  HUSEClassifier : torch.nn.Module\n","    The trained shared classification model \n","  \"\"\"\n","  \n","  # We pass the dataset to dataloader to break it and load data in batches\n","  data_loader = DataLoader(dataset,batch_size=config.batch_size,shuffle=True)\n","  \n","  # We collect all training parameters from all the three models \n","  all_parameters = list(TxtTower.parameters()) + list(ImgTower.parameters()) + list(HUSEClassifier.parameters())\n","  \n","  # We load all the models to the device we're using \n","  TxtTower.to(config.device)\n","  ImgTower.to(config.device)\n","  HUSEClassifier.to(config.device)\n","\n","  # We pass all the parameters to update and all the other hyperparameters \n","  # to RMSprop optimizer as mentioned in section 4.1.3 of the paper\n","  optimizer  = RMSprop(all_parameters, lr=config.lr, momentum=config.momentum)\n","  \n","  for epoch_num in range(config.num_epoch):\n","    \n","    print(\"--\" * 5)\n","    print(\"Epoch_num:\",epoch_num+1)\n","    \n","    epoch_loss = 0.0\n","    \n","    with progressbar.ProgressBar(max_value=len(data_loader)) as bar:\n","\n","      for batch_num,batch in enumerate(data_loader):\n","        \n","        # We fetch the batch data and move it to the device in use \n","        images_batch = batch[0].to(config.device)\n","        text_batch   = batch[1]\n","        class_name_labels    = batch[2]\n","        tf_idf_vectors_batch = torch.squeeze(batch[3]).to(config.device)\n","        class_idx_batch = torch.squeeze(batch[4][0]).to(config.device)\n","        \n","\n","        with torch.no_grad():\n","\n","          image_embeddings_batch = ImgEmbdGen.get_ImgEmbd(torch.squeeze(images_batch)).to(config.device)\n","          text_embeddings_batch  = torch.cat(list(map(TxtEmbdGen.get_TxtEmbd_bert,text_batch)),dim=0).to(config.device)\n","          final_text_embeddings_batch = torch.cat([text_embeddings_batch,tf_idf_vectors_batch],dim = 1).to(config.device)\n","        \n","        optimizer.zero_grad()\n","        TxtTower.zero_grad()\n","        ImgTower.zero_grad()\n","        HUSEClassifier.zero_grad()\n","\n","        with torch.set_grad_enabled(True):\n","\n","          universal_embedding_image = ImgTower(image_embeddings_batch)\n","          universal_embedding_text = TxtTower(final_text_embeddings_batch)\n","          HUSEClassifier_input = torch.cat([universal_embedding_image,universal_embedding_text],dim=1)\n","          HUSEClassifier_output = HUSEClassifier(HUSEClassifier_input)\n","        \n","        \n","        batch_labels_ndarr = np.array(class_name_labels).reshape(config.batch_size,1)\n","        batch_labels_ndarr = np.concatenate((batch_labels_ndarr, batch_labels_ndarr),axis=0)\n","        all_univeral_embeddings = torch.cat([universal_embedding_text,universal_embedding_image],dim = 0)\n","        semantic_loss = Loss.SemanticLoss(semantic_graph,classes_combs_map,all_univeral_embeddings,batch_labels_ndarr)\n","        cross_modal_loss = Loss.CrossModalLoss(universal_embedding_text,universal_embedding_image)\n","        classification_loss = Loss.ClassificationLoss(HUSEClassifier_output, class_idx_batch)\n","        total_loss = Loss.TotalLoss(classification_loss, semantic_loss, cross_modal_loss)\n","        total_loss.backward()\n","        optimizer.step()\n","        epoch_loss += total_loss * config.batch_size\n","        bar.update(batch_num)\n","\n","    print(f\"Training loss : {epoch_loss/len(data_loader.dataset)}\")\n","    print(\"--\" * 5)\n","\n","  return TxtTower, ImgTower, HUSEClassifier "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v7mS_OGQjSA8","colab_type":"code","colab":{}},"source":["train()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9GA2FkmtwkyF","colab_type":"text"},"source":["# Submitted By : Saahil Ali \n","## Email       : programmer290399@gmail.com\n","## LinkedIn    : https://www.linkedin.com/in/saahil-ali-290399/\n","## Contact No. : +91-9981789723"]},{"cell_type":"code","metadata":{"id":"rvzMxtjfxLfR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}